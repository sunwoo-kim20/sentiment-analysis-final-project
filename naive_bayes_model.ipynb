{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of datasets to use\n",
    "data = ['reviews_Amazon_Instant_Video_5.json.gz', 'reviews_Digital_Music_5.json.gz']\n",
    "\n",
    "# functions to read Amazon data into a pandas data frame\n",
    "def parse(path):\n",
    "    g = gzip.open(path, 'rb')\n",
    "    for l in g:\n",
    "        yield eval(l)\n",
    "\n",
    "def getDF(path):\n",
    "    i = 0\n",
    "    df = {}\n",
    "    for d in parse(path):\n",
    "        df[i] = d\n",
    "        i += 1\n",
    "    return pd.DataFrame.from_dict(df, orient='index')\n",
    "\n",
    "# function to concatenate multiple Amazon datasets\n",
    "def concatDF(data):\n",
    "    df = pd.DataFrame()\n",
    "    for dataset in data:\n",
    "        dftemp = getDF(dataset)\n",
    "        df = pd.concat([df, dftemp], axis=0)\n",
    "    # drop unneeded columns\n",
    "    df.drop(columns = ['reviewerID', 'asin', 'reviewerName', 'helpful', \\\n",
    "                       'summary', 'unixReviewTime', 'reviewTime'], inplace= True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of file paths to pass to functions for pandas conversion\n",
    "zipped_data = ['Resources/data/reviews_Amazon_Instant_Video_5.json.gz', 'Resources/data/reviews_Digital_Music_5.json.gz']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_reviews_df = concatDF(zipped_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I had big expectations because I love English ...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I highly recommend this series. It is a must f...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This one is a real snoozer. Don't believe anyt...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mysteries are interesting.  The tension betwee...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This show always is excellent, as far as briti...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          reviewText  overall\n",
       "0  I had big expectations because I love English ...      2.0\n",
       "1  I highly recommend this series. It is a must f...      5.0\n",
       "2  This one is a real snoozer. Don't believe anyt...      1.0\n",
       "3  Mysteries are interesting.  The tension betwee...      4.0\n",
       "4  This show always is excellent, as far as briti...      5.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_reviews_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Spark session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Sentiment_Analysis\").getOrCreate()\n",
    "# Enable Arrow-based columnar data transfers\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\skim3\\anaconda3\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:289: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:\n",
      "  PyArrow >= 1.0.0 must be installed; however, it was not found.\n",
      "Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+\n",
      "|          reviewText|overall|\n",
      "+--------------------+-------+\n",
      "|I had big expecta...|    2.0|\n",
      "|I highly recommen...|    5.0|\n",
      "|This one is a rea...|    1.0|\n",
      "|Mysteries are int...|    4.0|\n",
      "|This show always ...|    5.0|\n",
      "|I discovered this...|    5.0|\n",
      "|It beats watching...|    3.0|\n",
      "|There are many ep...|    3.0|\n",
      "|This is the best ...|    5.0|\n",
      "|Not bad.  Didn't ...|    3.0|\n",
      "|Funny, interestin...|    4.0|\n",
      "|I love the variet...|    4.0|\n",
      "|comedy is a matte...|    3.0|\n",
      "|if this had to do...|    3.0|\n",
      "|Watched it for Ke...|    5.0|\n",
      "|he's OK. His humo...|    2.0|\n",
      "|some comedians ar...|    3.0|\n",
      "|I only watched th...|    3.0|\n",
      "|Enjoyed some of t...|    5.0|\n",
      "|All the comedians...|    5.0|\n",
      "+--------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert pandas df to a spark df\n",
    "reviews_spark_df = spark.createDataFrame(clean_reviews_df)\n",
    "reviews_spark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+------+\n",
      "|          reviewText|overall|length|\n",
      "+--------------------+-------+------+\n",
      "|I had big expecta...|    2.0|   159|\n",
      "|I highly recommen...|    5.0|   186|\n",
      "|This one is a rea...|    1.0|   134|\n",
      "|Mysteries are int...|    4.0|   141|\n",
      "|This show always ...|    5.0|  1300|\n",
      "|I discovered this...|    5.0|  3039|\n",
      "|It beats watching...|    3.0|    99|\n",
      "|There are many ep...|    3.0|   348|\n",
      "|This is the best ...|    5.0|   186|\n",
      "|Not bad.  Didn't ...|    3.0|   127|\n",
      "|Funny, interestin...|    4.0|   111|\n",
      "|I love the variet...|    4.0|   221|\n",
      "|comedy is a matte...|    3.0|   112|\n",
      "|if this had to do...|    3.0|   112|\n",
      "|Watched it for Ke...|    5.0|   144|\n",
      "|he's OK. His humo...|    2.0|   132|\n",
      "|some comedians ar...|    3.0|   250|\n",
      "|I only watched th...|    3.0|   740|\n",
      "|Enjoyed some of t...|    5.0|   102|\n",
      "|All the comedians...|    5.0|   301|\n",
      "+--------------------+-------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import length\n",
    "# Create a length column to be used as a future feature \n",
    "reviews_spark_df = reviews_spark_df.withColumn('length', length(reviews_spark_df['reviewText']))\n",
    "reviews_spark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"reviewText\", outputCol=\"token_text\")\n",
    "stopremove = StopWordsRemover(inputCol='token_text',outputCol='stop_tokens')\n",
    "hashingTF = HashingTF(inputCol=\"stop_tokens\", outputCol='hash_token')\n",
    "idf = IDF(inputCol='hash_token', outputCol='idf_token')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.linalg import Vector\n",
    "\n",
    "# Create feature vectors\n",
    "vector_clean_up = VectorAssembler(inputCols=['idf_token', 'length'], outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "df_pipeline = Pipeline(stages=[tokenizer, stopremove, hashingTF, idf, vector_clean_up])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleaner = df_pipeline.fit(reviews_spark_df)\n",
    "final_tokenized_reviews_df = data_cleaner.transform(reviews_spark_df).select(\"token_text\", \"length\", \"stop_tokens\", \"idf_token\", \"features\", \"overall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+--------------------+--------------------+--------------------+-------+\n",
      "|          token_text|length|         stop_tokens|           idf_token|            features|overall|\n",
      "+--------------------+------+--------------------+--------------------+--------------------+-------+\n",
      "|[i, had, big, exp...|   159|[big, expectation...|(262144,[8287,310...|(262145,[8287,310...|    2.0|\n",
      "|[i, highly, recom...|   186|[highly, recommen...|(262144,[11472,19...|(262145,[11472,19...|    5.0|\n",
      "|[this, one, is, a...|   134|[one, real, snooz...|(262144,[21823,33...|(262145,[21823,33...|    1.0|\n",
      "|[mysteries, are, ...|   141|[mysteries, inter...|(262144,[23340,51...|(262145,[23340,51...|    4.0|\n",
      "|[this, show, alwa...|  1300|[show, always, ex...|(262144,[2325,410...|(262145,[2325,410...|    5.0|\n",
      "|[i, discovered, t...|  3039|[discovered, seri...|(262144,[1354,297...|(262145,[1354,297...|    5.0|\n",
      "|[it, beats, watch...|    99|[beats, watching,...|(262144,[63139,73...|(262145,[63139,73...|    3.0|\n",
      "|[there, are, many...|   348|[many, episodes, ...|(262144,[33123,51...|(262145,[33123,51...|    3.0|\n",
      "|[this, is, the, b...|   186|[best, best, come...|(262144,[21823,31...|(262145,[21823,31...|    5.0|\n",
      "|[not, bad., , did...|   127|[bad., , know, co...|(262144,[54961,55...|(262145,[54961,55...|    3.0|\n",
      "|[funny,, interest...|   111|[funny,, interest...|(262144,[2701,171...|(262145,[2701,171...|    4.0|\n",
      "|[i, love, the, va...|   221|[love, variety, c...|(262144,[8538,271...|(262145,[8538,271...|    4.0|\n",
      "|[comedy, is, a, m...|   112|[comedy, matter, ...|(262144,[63449,73...|(262145,[63449,73...|    3.0|\n",
      "|[if, this, had, t...|   112|[dat, phan,, hila...|(262144,[9747,273...|(262145,[9747,273...|    3.0|\n",
      "|[watched, it, for...|   144|[watched, kevin, ...|(262144,[24657,45...|(262145,[24657,45...|    5.0|\n",
      "|[he's, ok., his, ...|   132|[ok., humor, cons...|(262144,[40081,55...|(262145,[40081,55...|    2.0|\n",
      "|[some, comedians,...|   250|[comedians, good,...|(262144,[19736,43...|(262145,[19736,43...|    3.0|\n",
      "|[i, only, watched...|   740|[watched, wanda, ...|(262144,[8538,197...|(262145,[8538,197...|    3.0|\n",
      "|[enjoyed, some, o...|   102|[enjoyed, comedia...|(262144,[77365,97...|(262145,[77365,97...|    5.0|\n",
      "|[all, the, comedi...|   301|[comedians, hilar...|(262144,[10723,12...|(262145,[10723,12...|    5.0|\n",
      "+--------------------+------+--------------------+--------------------+--------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_tokenized_reviews_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|(262145,[8287,310...|  2.0|\n",
      "|(262145,[11472,19...|  5.0|\n",
      "|(262145,[21823,33...|  1.0|\n",
      "|(262145,[23340,51...|  4.0|\n",
      "|(262145,[2325,410...|  5.0|\n",
      "|(262145,[1354,297...|  5.0|\n",
      "|(262145,[63139,73...|  3.0|\n",
      "|(262145,[33123,51...|  3.0|\n",
      "|(262145,[21823,31...|  5.0|\n",
      "|(262145,[54961,55...|  3.0|\n",
      "|(262145,[2701,171...|  4.0|\n",
      "|(262145,[8538,271...|  4.0|\n",
      "|(262145,[63449,73...|  3.0|\n",
      "|(262145,[9747,273...|  3.0|\n",
      "|(262145,[24657,45...|  5.0|\n",
      "|(262145,[40081,55...|  2.0|\n",
      "|(262145,[19736,43...|  3.0|\n",
      "|(262145,[8538,197...|  3.0|\n",
      "|(262145,[77365,97...|  5.0|\n",
      "|(262145,[10723,12...|  5.0|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_data = final_tokenized_reviews_df.select('features', 'overall').withColumnRenamed('overall', 'label')\n",
    "input_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "# Break data down into a training set and a testing set\n",
    "train, test = input_data.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o217.fit.\n: org.apache.spark.SparkException: Job 5 cancelled because SparkContext was shut down\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1(DAGScheduler.scala:1085)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1$adapted(DAGScheduler.scala:1083)\r\n\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\r\n\tat org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:1083)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:2463)\r\n\tat org.apache.spark.util.EventLoop.stop(EventLoop.scala:84)\r\n\tat org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:2369)\r\n\tat org.apache.spark.SparkContext.$anonfun$stop$12(SparkContext.scala:2069)\r\n\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1419)\r\n\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2069)\r\n\tat org.apache.spark.SparkContext$$anon$3.run(SparkContext.scala:2018)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2217)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2261)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:390)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3696)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:2965)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\r\n\tat org.apache.spark.sql.Dataset.collect(Dataset.scala:2965)\r\n\tat org.apache.spark.ml.classification.NaiveBayes.trainDiscreteImpl(NaiveBayes.scala:193)\r\n\tat org.apache.spark.ml.classification.NaiveBayes.$anonfun$trainWithLabelCheck$1(NaiveBayes.scala:160)\r\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\r\n\tat scala.util.Try$.apply(Try.scala:213)\r\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\r\n\tat org.apache.spark.ml.classification.NaiveBayes.trainWithLabelCheck(NaiveBayes.scala:144)\r\n\tat org.apache.spark.ml.classification.NaiveBayes.train(NaiveBayes.scala:133)\r\n\tat org.apache.spark.ml.classification.NaiveBayes.train(NaiveBayes.scala:95)\r\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:151)\r\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:115)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-d640c85492cf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Create a Naive Bayes model and fit training data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mnb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNaiveBayes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mpredictor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\ml\\base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    159\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 161\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    162\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\ml\\wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 335\u001b[1;33m         \u001b[0mjava_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    336\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    337\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\ml\\wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    330\u001b[0m         \"\"\"\n\u001b[0;32m    331\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 332\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    333\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1304\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m                 raise Py4JJavaError(\n\u001b[0m\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o217.fit.\n: org.apache.spark.SparkException: Job 5 cancelled because SparkContext was shut down\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1(DAGScheduler.scala:1085)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1$adapted(DAGScheduler.scala:1083)\r\n\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\r\n\tat org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:1083)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:2463)\r\n\tat org.apache.spark.util.EventLoop.stop(EventLoop.scala:84)\r\n\tat org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:2369)\r\n\tat org.apache.spark.SparkContext.$anonfun$stop$12(SparkContext.scala:2069)\r\n\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1419)\r\n\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2069)\r\n\tat org.apache.spark.SparkContext$$anon$3.run(SparkContext.scala:2018)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2217)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2261)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:390)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3696)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:2965)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\r\n\tat org.apache.spark.sql.Dataset.collect(Dataset.scala:2965)\r\n\tat org.apache.spark.ml.classification.NaiveBayes.trainDiscreteImpl(NaiveBayes.scala:193)\r\n\tat org.apache.spark.ml.classification.NaiveBayes.$anonfun$trainWithLabelCheck$1(NaiveBayes.scala:160)\r\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\r\n\tat scala.util.Try$.apply(Try.scala:213)\r\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\r\n\tat org.apache.spark.ml.classification.NaiveBayes.trainWithLabelCheck(NaiveBayes.scala:144)\r\n\tat org.apache.spark.ml.classification.NaiveBayes.train(NaiveBayes.scala:133)\r\n\tat org.apache.spark.ml.classification.NaiveBayes.train(NaiveBayes.scala:95)\r\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:151)\r\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:115)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception happened during processing of request from ('127.0.0.1', 53797)\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\skim3\\anaconda3\\lib\\socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"C:\\Users\\skim3\\anaconda3\\lib\\socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"C:\\Users\\skim3\\anaconda3\\lib\\socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"C:\\Users\\skim3\\anaconda3\\lib\\socketserver.py\", line 720, in __init__\n",
      "    self.handle()\n",
      "  File \"C:\\Users\\skim3\\anaconda3\\lib\\site-packages\\pyspark\\accumulators.py\", line 262, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"C:\\Users\\skim3\\anaconda3\\lib\\site-packages\\pyspark\\accumulators.py\", line 235, in poll\n",
      "    if func():\n",
      "  File \"C:\\Users\\skim3\\anaconda3\\lib\\site-packages\\pyspark\\accumulators.py\", line 239, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"C:\\Users\\skim3\\anaconda3\\lib\\site-packages\\pyspark\\serializers.py\", line 562, in read_int\n",
      "    length = stream.read(4)\n",
      "  File \"C:\\Users\\skim3\\anaconda3\\lib\\socket.py\", line 669, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Create a Naive Bayes model and fit training data\n",
    "nb = NaiveBayes()\n",
    "predictor = nb.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = predictor.transform(test)\n",
    "test_results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the Class Evaluator for a cleaner description\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "acc_eval = MulticlassClassificationEvaluator()\n",
    "acc = acc_eval.evaluate(test_results)\n",
    "print(\"Accuracy of model: %f\" % acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
